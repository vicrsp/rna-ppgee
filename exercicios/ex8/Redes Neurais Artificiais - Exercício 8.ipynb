{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('dev-rna': conda)",
   "metadata": {
    "interpreter": {
     "hash": "e306745eb04e6255f5ef90bd7aabf87d10dafa8dd8a5433f87810e7e6feeffc5"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<strong>Aluno</strong>: Victor São Paulo Ruela\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF:\n",
    "    def __init__(self, p, center_estimation='kmeans'):\n",
    "        self.p = p\n",
    "        self.center_estimation = center_estimation\n",
    "\n",
    "    def apply_transformation(self, X):\n",
    "        N, n = X.shape\n",
    "        H = np.zeros((N, self.p))\n",
    "        for j in range(N):\n",
    "            for i in range(self.p):\n",
    "                mi = self.centers_[i, :]\n",
    "                covi = self.cov_[i] + 0.001 * np.eye(n)\n",
    "                H[j, i] = self.gaussian_kernel(X[j, :], mi, covi, n)\n",
    "        return H\n",
    "\n",
    "    def predict(self, X):\n",
    "        N, _ = X.shape\n",
    "\n",
    "        H = self.apply_transformation(X)\n",
    "        H_aug = np.hstack((np.ones((N, 1)), H))\n",
    "        yhat = H_aug @ self.coef_\n",
    "\n",
    "        return yhat\n",
    "\n",
    "    def gaussian_kernel(self, X, m, K, n):\n",
    "        if n == 1:\n",
    "            r = np.sqrt(float(K))\n",
    "            px = (1/(np.sqrt(2*np.pi*r*r)))*np.exp(-0.5 * (float(X-m)/r)**2)\n",
    "            return px\n",
    "        else:\n",
    "            center_distance = (X - m).reshape(-1, 1)\n",
    "            normalization_factor = np.sqrt(((2*np.pi)**n)*np.linalg.det(K))\n",
    "            dist = float(\n",
    "                np.exp(-0.5 * (center_distance.T @ (np.linalg.inv(K)) @ center_distance)))\n",
    "            return dist / normalization_factor\n",
    "            \n",
    "    def make_centers(self, X):\n",
    "        N, n = X.shape\n",
    "\n",
    "        if(self.center_estimation == 'kmeans'):\n",
    "            kmeans = KMeans(n_clusters=self.p).fit(X)\n",
    "            self.centers_ = kmeans.cluster_centers_\n",
    "            # estimate covariance matrix for all centers\n",
    "            clusters = kmeans.predict(X)\n",
    "            covlist = []\n",
    "            for i in range(self.p):\n",
    "                xci = X[clusters == i, :]\n",
    "                covi = np.cov(xci, rowvar=False) if n > 1 else np.asarray(\n",
    "                    np.var(xci))\n",
    "                covlist.append(covi)\n",
    "            self.cov_ = covlist\n",
    "        elif (self.center_estimation == 'random'):\n",
    "            random_idx = random.sample(range(N), self.p)\n",
    "            # define the maximum radius possible from the data\n",
    "            data_center = np.mean(X, axis=0).reshape(1,-1)\n",
    "            max_radius = np.max(np.linalg.norm(X - data_center, axis=1))\n",
    "            # Now define random radius values\n",
    "            random_radius = np.random.uniform(max_radius * 0.3, max_radius * 0.7, self.p)\n",
    "\n",
    "            covlist = []\n",
    "            centers = []\n",
    "            for i in range(self.p):\n",
    "                # Get the two random points from list\n",
    "                center = X[random_idx[i], :]\n",
    "                points_within_radius = np.linalg.norm(X - center, axis=1) <= random_radius[i]\n",
    "                xci = X[points_within_radius, :]\n",
    "                covi = np.cov(xci, rowvar=False) if n > 1 else np.asarray(np.var(xci))\n",
    "\n",
    "                covlist.append(covi)\n",
    "                centers.append(center)\n",
    "            self.cov_ = covlist\n",
    "            self.centers_ = np.asarray(centers)\n",
    "\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # check X, y consistency\n",
    "        #X, y = check_X_y(X, y, accept_sparse=True)\n",
    "        N, _ = X.shape\n",
    "\n",
    "        # define centers\n",
    "        self.make_centers(X)\n",
    "        # calculate H matrix\n",
    "        H = self.apply_transformation(X)\n",
    "\n",
    "        H_aug = np.hstack((np.ones((N, 1)), H))\n",
    "        self.coef_ = (np.linalg.inv(H_aug.T @ H_aug) @ H_aug.T) @ y\n",
    "        self.H_ = H\n",
    "\n",
    "        return self\n",
    "\n",
    "class ELM:\n",
    "    def __init__(self, p=5):\n",
    "        self.p = p\n",
    "\n",
    "    def predict(self, X):\n",
    "        N, _ = X.shape\n",
    "        x_aug = np.hstack((-np.ones((N, 1)), X))\n",
    "        H = np.tanh(x_aug @ self.Z_)\n",
    "        return H @ self.coef_\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # augment X\n",
    "        N, n = X.shape\n",
    "        x_aug = np.hstack((-np.ones((N, 1)), X))\n",
    "        # create initial Z matrix\n",
    "        Z = np.random.uniform(-0.5, 0.5, (n+1, self.p))\n",
    "        # apply activation function: tanh\n",
    "        H = np.tanh(x_aug @ Z)\n",
    "        # calculate the weights\n",
    "        w = np.linalg.pinv(H) @ y\n",
    "        # store fitted data\n",
    "        self.coef_ = w\n",
    "        self.Z_ = Z\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the breast cancer data\n",
    "X_bc, y_bc = load_breast_cancer(return_X_y = True)\n",
    "# convert the classes to -1 or 1\n",
    "y_bc = pd.Series(y_bc).map({0:-1,1:1}).to_numpy()\n",
    "# scale the data\n",
    "scaler_bc = MinMaxScaler()\n",
    "X_bc = scaler_bc.fit_transform(X_bc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the statlog(heart) dataset\n",
    "heart_df = pd.read_csv('heart.dat', sep=' ', header=None)\n",
    "X_hd, y_hd = heart_df.iloc[:,:-1].to_numpy(), heart_df.iloc[:,-1].map({2:1, 1:-1}).to_numpy()\n",
    "# scale the data\n",
    "scaler_hd = MinMaxScaler()\n",
    "X_hd = scaler_hd.fit_transform(X_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test, neurons, reps=10, model='rbf-random'):\n",
    "    accuracy_results = []\n",
    "    accuracy_results_train = []\n",
    "    for p in tqdm(neurons):\n",
    "        accuracy_test = []\n",
    "        accuracy_train = []\n",
    "        for _ in range(reps):\n",
    "            if(model == 'rbf-random'):\n",
    "                classifier = RBF(p=p,center_estimation='random').fit(X_train, y_train)\n",
    "            elif(model == 'rbf-kmeans'):\n",
    "                classifier = RBF(p=p,center_estimation='kmeans').fit(X_train, y_train)\n",
    "            elif(model == 'elm'):\n",
    "                classifier = ELM(p=p).fit(X_train, y_train)\n",
    "            else:\n",
    "                raise ValueError('Invalid model name')\n",
    "            \n",
    "            yhat = np.sign(classifier.predict(X_test))\n",
    "            yhat_train = np.sign(classifier.predict(X_train))\n",
    "            # accuracy\n",
    "            accuracy_test.append(accuracy_score(y_test, yhat))\n",
    "            accuracy_train.append(accuracy_score(y_train, yhat_train))\n",
    "            \n",
    "        accuracy_results.append(np.mean(accuracy_test))\n",
    "        accuracy_results_train.append(np.mean(accuracy_train))\n",
    "    return accuracy_results, accuracy_results_train\n",
    "\n",
    "def run_experiment(X, y, N=30, neurons=[5,10,30,50,100], model='rbf-random', plot=True, datasets=None):\n",
    "    experiment_values_test = []\n",
    "    experiment_values_train = []\n",
    "    data_sets = {}\n",
    "    for i in tqdm(range(N)):\n",
    "        # split the data\n",
    "        if(datasets == None):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)    \n",
    "        else:\n",
    "            X_train, y_train, X_test, y_test = datasets[i]\n",
    "        \n",
    "        data_sets[i] = (X_train, y_train, X_test, y_test)\n",
    "        # run for every neuron\n",
    "        test_values, train_values = train_model(X_train, y_train, X_test, y_test, neurons, reps=1, model=model)\n",
    "\n",
    "        experiment_values_test.append(test_values)\n",
    "        experiment_values_train.append(train_values)\n",
    "\n",
    "    def convert_results(res, set): \n",
    "        df = pd.DataFrame(res, columns=neurons).melt(value_vars=neurons,value_name='Acurácia', var_name='Neurônios')\n",
    "        df['Conjunto'] = set\n",
    "        return df\n",
    "\n",
    "    train_values_df = convert_results(experiment_values_train, 'Treino')\n",
    "    test_values_df = convert_results(experiment_values_test, 'Teste')\n",
    "    \n",
    "    experiment_values_df = pd.concat([train_values_df, test_values_df], ignore_index=True)\n",
    "    clear_output(wait=True)\n",
    "    if (plot==True):\n",
    "        # Plot the accuracy boxplots\n",
    "        plt.figure(figsize=(8,5))\n",
    "        sns.boxplot(data=experiment_values_df, x='Conjunto',y='Acurácia', hue='Neurônios', showfliers=False)\n",
    "       \n",
    "\n",
    "    return experiment_values_df, data_sets "
   ]
  },
  {
   "source": [
    "## Parte 1: Base de dados Breast Cancer (Diagnostic)\n",
    "\n",
    "Para este exercício será feita uma análise do algoritmo RBF com centros aleatórios, k-médias e o algoritmo ELM. O objetivo é avaliar o impacto do número de nerônios na sua capacidade de generalização e verifica, a partir do seguinte experimento:\n",
    "\n",
    "Nº de neurônios: $[5,10,30,50,100,150]$\n",
    "\n",
    "* 1) Separar o conjunto de testes entre treinamento \n",
    "* 2) Para cada quantidade de neurônios na lista, treinar o RBF 5 vezes e obter a média dos resultados\n",
    "* 4) Repetir os passos 1-2 30 vezes.\n",
    "\n",
    "Uma rotina foi criada para a execução deste experimento, cujos resultados são exibidos abaixo no gráfico boxplot e tabela com a média e desvio padrão obtidos para a acurácia."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.86it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.25it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.69s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:08<00:02,  2.83s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:20<00:00,  4.07s/it]\n",
      "  5%|▌         | 1/20 [00:20<06:26, 20.35s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.79it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.14it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.86s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:10<00:03,  3.33s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:22<00:00,  4.57s/it]\n",
      " 10%|█         | 2/20 [00:43<06:32, 21.82s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.87it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.09it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:05<00:04,  2.25s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:11<00:03,  3.88s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:22<00:00,  4.58s/it]\n",
      " 15%|█▌        | 3/20 [01:06<06:19, 22.32s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.92it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.27it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.70s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:08<00:02,  2.82s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.58s/it]\n",
      " 20%|██        | 4/20 [01:24<05:29, 20.57s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.92it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.26it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.66s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:08<00:02,  2.78s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:19<00:00,  3.98s/it]\n",
      " 25%|██▌       | 5/20 [01:43<05:05, 20.33s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.82it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.17it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:05<00:04,  2.08s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:10<00:03,  3.38s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:22<00:00,  4.41s/it]\n",
      " 30%|███       | 6/20 [02:05<04:52, 20.92s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.78it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.21it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.77s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:09<00:02,  2.94s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:18<00:00,  3.65s/it]\n",
      " 35%|███▌      | 7/20 [02:24<04:20, 20.05s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.90it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:01<00:02,  1.27it/s]\u001b[A\n",
      " 60%|██████    | 3/5 [00:04<00:03,  1.67s/it]\u001b[A\n",
      " 80%|████████  | 4/5 [00:08<00:02,  2.78s/it]\u001b[A\n",
      "100%|██████████| 5/5 [00:17<00:00,  3.53s/it]\n",
      " 40%|████      | 8/20 [02:41<03:51, 19.29s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:02,  1.85it/s]\u001b[A\n",
      " 40%|████      | 2/5 [00:02<00:04,  1.34s/it]\n",
      " 40%|████      | 8/20 [02:44<04:06, 20.57s/it]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-50d302a96fa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_bc_rbfrandom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets_bc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_bc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_bc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rbf-random'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-cbfe9c63f2a1>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(X, y, N, neurons, model, plot, datasets)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mdata_sets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# run for every neuron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtest_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mexperiment_values_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-cbfe9c63f2a1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X_train, y_train, X_test, y_test, neurons, reps, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rbf-random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                 \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRBF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcenter_estimation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rbf-kmeans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRBF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcenter_estimation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'kmeans'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9941f1afdaf2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_centers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# calculate H matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_transformation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mH_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9941f1afdaf2>\u001b[0m in \u001b[0;36mapply_transformation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mmi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mcovi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mH\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9941f1afdaf2>\u001b[0m in \u001b[0;36mgaussian_kernel\u001b[0;34m(self, X, m, K, n)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mnormalization_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             dist = float(\n\u001b[0;32m---> 34\u001b[0;31m                 np.exp(-0.5 * (center_distance.T @ (np.linalg.inv(K)) @ center_distance)))\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnormalization_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dev-rna/lib/python3.8/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results_bc_rbfrandom, datasets_bc = run_experiment(X_bc, y_bc,neurons=[5,10,30,50,100], N=20, model='rbf-random',plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_bc_rbfkmeans, _ = run_experiment(X_bc, y_bc,neurons=[5,10,30,50,100], N=20, model='rbf-kmeans', datasets=datasets_bc, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_bc_elm, _ = run_experiment(X_bc, y_bc,neurons=[5,10,30,50,100], N=20, model='elm', datasets=datasets_bc,plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bc_elm['Modelo'] = 'ELM'\n",
    "results_bc_rbfrandom['Modelo'] = 'RBF - Aleatório'\n",
    "results_bc_rbfkmeans['Modelo'] = 'RBF - k-means'\n",
    "\n",
    "results = pd.concat([results_bc_elm, results_bc_rbfkmeans, results_bc_rbfrandom], ignore_index=True)\n",
    "\n",
    "#plt.figure(figsize=(8,5))\n",
    "sns.catplot(data=results, x='Conjunto',y='Acurácia', hue='Neurônios', col='Modelo', kind='box', showfliers=False)\n",
    "\n",
    "display('Tabela: Acurácia Média e Desvio Padrão')\n",
    "display(results.groupby(['Neurônios','Conjunto','Modelo'])['Acurácia'].agg([np.mean, np.std]))"
   ]
  },
  {
   "source": [
    "A partir dos resultados acima, é possível concluir que:\n",
    "\n",
    "* A acurácia média de treinamento cresce proporcionalmente com o número de neurônios, conforme esperado. Entretanto, se observamos a acurácia de teste, é possível notar ela se mantém praticamente inalterada e com média bem próxima independente do número de neurônios.\n",
    "\n",
    "Durante os testes, foi observado que o esforço computacional cresce muito à medida em que aumentamos a quantidade de neurônios na camada escondida. Isso fez com que fosse necessário limitar esta grandez para que o relatório fosse executado em tempo hábil. \n",
    "\n",
    "A seguir são exibidos os resultados do mesmo experimento considerando o RBF com inicialização via k-means.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Parte 2: Base Statlog (Heart)\n",
    "\n",
    "A mesma análise anterior é feita considerando a base de dados Statlog (Heart). O mesmo experimento é executado, considerando a rotina desenvolvida anteriormente. Os resultados são exibidos e analisados abaixo."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_hd_rbfrandom, datasets_hd = run_experiment(X_hd, y_hd,neurons=[5,10,30,50,100], N=30, model='rbf-random',plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_hd_rbfkmeans, _ = run_experiment(X_hd, y_hd,neurons=[5,10,30,50,100], N=30, model='rbf-kmeans',plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_hd_elm, _ = run_experiment(X_hd, y_hd,neurons=[5,10,30,50,100], N=30, model='elm',plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_hd_elm['Modelo'] = 'ELM'\n",
    "results_hd_rbfrandom['Modelo'] = 'RBF - Aleatório'\n",
    "results_hd_rbfkmeans['Modelo'] = 'RBF - k-means'\n",
    "\n",
    "results = pd.concat([results_hd_elm, results_hd_rbfkmeans, results_hd_rbfrandom], ignore_index=True)\n",
    "\n",
    "sns.catplot(data=results, x='Conjunto',y='Acurácia', hue='Neurônios', col='Modelo', kind='box', showfliers=False)\n",
    "\n",
    "display('Tabela: Acurácia Média e Desvio Padrão')\n",
    "display(results.groupby(['Neurônios','Conjunto','Modelo'])['Acurácia'].agg([np.mean, np.std]))"
   ]
  },
  {
   "source": [
    "A partir dos resultados acima, é possível concluir que:\n",
    "\n",
    "* Os resultados são bem similares aos encontrados para a base de dados anterior: a acurácia média de treinamento cresce proporcionalmente com o número de neurônios, porém a acurácia de teste começa a reduzir com aproximadamente 30 neurônios.\n",
    "* Nota-se que a acurácia de teste chega ao seu máximo considerando uma quantidade em torno de 30 neurônios. \n",
    "* Mais uma vez, fica claro que a quantidade de neurônios é um fator importante para se obter uma boa generalização com ELMs.\n",
    "\n",
    "A seguir são exibidos os resultados do mesmo experimento considerando o RBF com inicialização via k-means."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Parte 3: Comparação com o ELM\n",
    "\n",
    "Um experimento similiar ao anterior é desenvolvido a seguir para avaliar o algoritmo ELM. Serão considerados os mesmos conjuntos de dados utilizados em cada repetição do experimento do RBF, de forma a se ter uma comparação mais justa entre os algoritmos.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conclusões\n",
    "\n",
    "Por meio dos experimentos realizados neste exercício, foi possível constatar que ELMs são capazes de obter um bom desempenho nas bases de dados avaliadas. Entretanto, é importante notar que a escolha da quantidade de neurônios é um hiper-parâmetro que deve ser bem escolhido para se conseguir obter uma boa generalização. Os resultados mostram que o aumento da quantidade de neurônios irá levar ao over-fitting deste modelo. \n",
    "\n",
    "Outro resultado importante foi que o Perceptron, embora seja um modelo mais simples, foi capaz de obter resultados similares ao do ELM, com a vantagem de não necessitar do ajuste de nenhum hiper-parâmetro para obter uma boa generalização. Vale a pena ressaltar que as bases de dados consideradas são linermente separáveis, de forma que não podemos extrapolar este resultado para problemas que não possuem esta característica."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}